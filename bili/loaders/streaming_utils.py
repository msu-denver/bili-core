"""Framework-agnostic streaming utilities for LangGraph agents.

Provides simple functions to stream token-level output from any
compiled LangGraph agent without requiring Flask, Streamlit, or
any other framework.

Usage::

    from bili.loaders.langchain_loader import build_agent_graph
    from bili.loaders.streaming_utils import stream_agent

    agent = build_agent_graph(...)

    # Synchronous streaming â€” works anywhere
    for token in stream_agent(agent, "What is the weather?", thread_id="user1"):
        print(token, end="", flush=True)

    # Async streaming
    async for token in astream_agent(agent, "Hello", thread_id="user1"):
        print(token, end="", flush=True)
"""

import logging
from typing import Any, AsyncGenerator, Dict, Generator, Optional

from langchain_core.messages import AIMessageChunk, HumanMessage

LOGGER = logging.getLogger(__name__)


def stream_agent(
    agent: Any,
    prompt: str,
    thread_id: Optional[str] = None,
    input_overrides: Optional[Dict[str, Any]] = None,
) -> Generator[str, None, None]:
    """Stream tokens from a compiled LangGraph agent.

    Uses LangGraph's ``.stream(stream_mode="messages")`` to yield
    individual token strings as they arrive from the LLM.

    Args:
        agent: A compiled LangGraph (``CompiledStateGraph``).
        prompt: The user prompt string.
        thread_id: Optional thread ID for checkpointed conversations.
        input_overrides: Optional extra keys to merge into the input
            state dict (e.g. ``{"verbose": True}``).

    Yields:
        Token strings as they are generated by the LLM.

    Example::

        for token in stream_agent(agent, "Hello"):
            print(token, end="", flush=True)
        print()  # newline after streaming completes
    """
    config = _build_config(thread_id)
    input_state = _build_input(prompt, input_overrides)

    try:
        for chunk in agent.stream(input_state, config=config, stream_mode="messages"):
            token = _extract_token(chunk)
            if token:
                yield token
    except Exception:  # pylint: disable=broad-exception-caught
        LOGGER.error("stream_agent failed", exc_info=True)
        yield "\n\n[Error: Streaming response failed.]"


async def astream_agent(
    agent: Any,
    prompt: str,
    thread_id: Optional[str] = None,
    input_overrides: Optional[Dict[str, Any]] = None,
) -> AsyncGenerator[str, None]:
    """Async stream tokens from a compiled LangGraph agent.

    Uses LangGraph's ``.astream_events(version="v2")`` to yield
    individual token strings with full async support.

    Args:
        agent: A compiled LangGraph (``CompiledStateGraph``).
        prompt: The user prompt string.
        thread_id: Optional thread ID for checkpointed conversations.
        input_overrides: Optional extra keys to merge into the input
            state dict.

    Yields:
        Token strings as they are generated by the LLM.

    Example::

        async for token in astream_agent(agent, "Hello"):
            print(token, end="", flush=True)
    """
    config = _build_config(thread_id)
    input_state = _build_input(prompt, input_overrides)

    try:
        async for event in agent.astream_events(
            input_state, config=config, version="v2"
        ):
            if event.get("event") == "on_chat_model_stream":
                chunk = event.get("data", {}).get("chunk")
                if chunk is not None:
                    content = getattr(chunk, "content", "")
                    if content:
                        yield content
    except Exception:  # pylint: disable=broad-exception-caught
        LOGGER.error("astream_agent failed", exc_info=True)
        yield "\n\n[Error: Async streaming response failed.]"


def invoke_agent(
    agent: Any,
    prompt: str,
    thread_id: Optional[str] = None,
    input_overrides: Optional[Dict[str, Any]] = None,
) -> str:
    """Invoke a compiled LangGraph agent and return the response text.

    Convenience function that wraps the standard ``.invoke()`` call
    and extracts the final AI message content.

    Args:
        agent: A compiled LangGraph (``CompiledStateGraph``).
        prompt: The user prompt string.
        thread_id: Optional thread ID for checkpointed conversations.
        input_overrides: Optional extra keys to merge into the input
            state dict.

    Returns:
        The content of the final AI message, or an error string.

    Example::

        response = invoke_agent(agent, "What is 2+2?")
        print(response)
    """
    config = _build_config(thread_id)
    input_state = _build_input(prompt, input_overrides)

    try:
        result = agent.invoke(input_state, config=config)

        if isinstance(result, dict) and "messages" in result:
            final_msg = result["messages"][-1]
            return getattr(final_msg, "content", str(final_msg))

        return "No response or invalid format."
    except Exception:  # pylint: disable=broad-exception-caught
        LOGGER.error("invoke_agent failed", exc_info=True)
        return "[Error: Agent invocation failed.]"


# ======================================================================
# Internal helpers
# ======================================================================


def _build_config(thread_id: Optional[str]) -> Dict[str, Any]:
    """Build LangGraph invoke config with optional thread_id."""
    if thread_id:
        return {"configurable": {"thread_id": thread_id}}
    return {}


def _build_input(prompt: str, overrides: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    """Build initial state dict from prompt and optional overrides."""
    state: Dict[str, Any] = {
        "messages": [HumanMessage(content=prompt)],
        "verbose": False,
    }
    if overrides:
        state.update(overrides)
    return state


def _extract_token(chunk: Any) -> Optional[str]:
    """Extract token content from a LangGraph stream chunk.

    ``stream_mode="messages"`` yields ``(message_chunk, metadata)``
    tuples.  This function handles that format and falls back to
    direct attribute access.
    """
    if isinstance(chunk, tuple) and len(chunk) >= 1:
        msg_chunk = chunk[0]
        if not isinstance(msg_chunk, AIMessageChunk):
            return None
        content = getattr(msg_chunk, "content", "")
        return content if content else None

    if hasattr(chunk, "content") and chunk.content:
        return chunk.content

    return None
